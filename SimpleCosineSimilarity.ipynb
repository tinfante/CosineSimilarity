{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['natural', 'language', 'processing', 'with', 'python'],\n",
       " ['handbook', 'of', 'natural', 'language', 'processing'],\n",
       " ['learning',\n",
       "  'ipython',\n",
       "  'for',\n",
       "  'interactive',\n",
       "  'computing',\n",
       "  'and',\n",
       "  'data',\n",
       "  'visualization']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "docs = [\"Natural Language Processing with Python\",\n",
    "        \"Handbook of Natural Language Processing\",\n",
    "        \"Learning IPython for Interactive Computing and Data Visualization\"]\n",
    "\n",
    "# We have three documents (book titles), first two are about a similar\n",
    "# subject, last one is different. Lets tokenize the documents and\n",
    "# normalize them to lowercase.\n",
    "\n",
    "tokenized_documents = [re.findall(r'\\w+', d.lower()) for d in docs]\n",
    "tokenized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'computing',\n",
       " 'data',\n",
       " 'for',\n",
       " 'handbook',\n",
       " 'interactive',\n",
       " 'ipython',\n",
       " 'language',\n",
       " 'learning',\n",
       " 'natural',\n",
       " 'of',\n",
       " 'processing',\n",
       " 'python',\n",
       " 'visualization',\n",
       " 'with']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = sorted(set(sum(tokenized_documents, [])))\n",
    "\n",
    "# our lexicon or vocabulary looks like this.\n",
    "\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('and', 0),\n",
       "             ('computing', 0),\n",
       "             ('data', 0),\n",
       "             ('for', 0),\n",
       "             ('handbook', 0),\n",
       "             ('interactive', 0),\n",
       "             ('ipython', 0),\n",
       "             ('language', 0),\n",
       "             ('learning', 0),\n",
       "             ('natural', 0),\n",
       "             ('of', 0),\n",
       "             ('processing', 0),\n",
       "             ('python', 0),\n",
       "             ('visualization', 0),\n",
       "             ('with', 0)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "vector_template = OrderedDict((token, 0) for token in lexicon)\n",
    "\n",
    "# our vector template looks like this.\n",
    "\n",
    "vector_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('and', 0),\n",
       "              ('computing', 0),\n",
       "              ('data', 0),\n",
       "              ('for', 0),\n",
       "              ('handbook', 0),\n",
       "              ('interactive', 0),\n",
       "              ('ipython', 0),\n",
       "              ('language', 0.1),\n",
       "              ('learning', 0),\n",
       "              ('natural', 0.1),\n",
       "              ('of', 0),\n",
       "              ('processing', 0.1),\n",
       "              ('python', 0.2),\n",
       "              ('visualization', 0),\n",
       "              ('with', 0.2)]),\n",
       " OrderedDict([('and', 0),\n",
       "              ('computing', 0),\n",
       "              ('data', 0),\n",
       "              ('for', 0),\n",
       "              ('handbook', 0.2),\n",
       "              ('interactive', 0),\n",
       "              ('ipython', 0),\n",
       "              ('language', 0.1),\n",
       "              ('learning', 0),\n",
       "              ('natural', 0.1),\n",
       "              ('of', 0.2),\n",
       "              ('processing', 0.1),\n",
       "              ('python', 0),\n",
       "              ('visualization', 0),\n",
       "              ('with', 0)]),\n",
       " OrderedDict([('and', 0.2),\n",
       "              ('computing', 0.2),\n",
       "              ('data', 0.2),\n",
       "              ('for', 0.2),\n",
       "              ('handbook', 0),\n",
       "              ('interactive', 0.2),\n",
       "              ('ipython', 0.2),\n",
       "              ('language', 0),\n",
       "              ('learning', 0.2),\n",
       "              ('natural', 0),\n",
       "              ('of', 0),\n",
       "              ('processing', 0),\n",
       "              ('python', 0),\n",
       "              ('visualization', 0.2),\n",
       "              ('with', 0)])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "from collections import Counter\n",
    "\n",
    "doc_tfidf_vectors = []\n",
    "for doc_tokens in tokenized_documents:\n",
    "    vec = copy.copy(vector_template)\n",
    "    token_counts = Counter(doc_tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        docs_containing_key = 0\n",
    "        for _doc_tokens in tokenized_documents:\n",
    "            if key in _doc_tokens:\n",
    "                docs_containing_key += 1\n",
    "        tf = value / len(lexicon)  # normalized by vocab size here?\n",
    "        if docs_containing_key:\n",
    "            idf = len(tokenized_documents) / docs_containing_key\n",
    "        else:\n",
    "            idf = 0\n",
    "        vec[key] = tf * idf\n",
    "    doc_tfidf_vectors.append(vec)\n",
    "    \n",
    "# and our document vectors are the following\n",
    "\n",
    "doc_tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2727272727272727\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    vec1 = list(vec1.values())\n",
    "    vec2 = list(vec2.values())\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    return dot_prod / (mag_1 * mag_2)\n",
    "\n",
    "# Lets compare the first document to the other two. As expected\n",
    "# document 1 and 2 have some similarity, documents 1 and 3 have\n",
    "# 0 similarity since they don't share any words.\n",
    "\n",
    "print(cosine_sim(doc_tfidf_vectors[0], doc_tfidf_vectors[1]))\n",
    "print(cosine_sim(doc_tfidf_vectors[0], doc_tfidf_vectors[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t0\t1\t2\n",
      "0\t1.0\t0.27\t0.0\t\n",
      "1\t0.27\t1.0\t0.0\t\n",
      "2\t0.0\t0.0\t1.0\t\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t0\\t1\\t2\")\n",
    "for r, doc1 in enumerate(doc_tfidf_vectors):\n",
    "    print(r, end='\\t')\n",
    "    for c, doc2 in enumerate(doc_tfidf_vectors):\n",
    "        print(round(cosine_sim(doc1, doc2), 2), end='\\t')\n",
    "    print()\n",
    "\n",
    "# And lets compare every document to the other documents\n",
    "# in our toy corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.7905694150420947\n"
     ]
    }
   ],
   "source": [
    "query = \"IPython Interactive Computing and Visualization Cookbook\"\n",
    "query_vec = copy.copy(vector_template)\n",
    "tokens = re.findall(r'\\w+', query.lower())\n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "for key, value in token_counts.items():\n",
    "    docs_containing_key = 0\n",
    "    for _doc in tokenized_documents:\n",
    "        if key in _doc:\n",
    "            docs_containing_key += 1\n",
    "    if docs_containing_key == 0:\n",
    "        continue\n",
    "    tf = value / len(tokens)  # normalized by doc word length here?\n",
    "    idf = len(tokenized_documents) / docs_containing_key\n",
    "    query_vec[key] = tf * idf\n",
    "    \n",
    "# And finally, lets query our model with a new document.\n",
    "# Unsurprisingly, this new book title is only similar\n",
    "# to the last document in our collection.\n",
    "\n",
    "print(cosine_sim(query_vec, doc_tfidf_vectors[0]))\n",
    "print(cosine_sim(query_vec, doc_tfidf_vectors[1]))\n",
    "print(cosine_sim(query_vec, doc_tfidf_vectors[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
